一、马尔科夫过程

对于一个分布P(x)，马尔科夫过程是一种实现从这个分布中采样的方法。具体方式为对于这个分布，找到一种只与初末状态相关的随机过程。即对于分布中任意一个状态，在下一个时刻将按照固定的转移概率转移到另一个状态。我们定义从 $x_i$ 状态在下一个时刻转移到 $x_j$ 状态的概率为：


$$
P_{j,i}=P(x_j^{(t+1)}|x_i^{(t)})
$$


这种随机变量每个时刻按照固定的转移概率不断在不同状态间转移的随机过程就是马尔科夫过程。

若记 $\pi_i^{(t)}$ 表示随机变量在t时刻恰好位于 $x_i$ 状态的概率，则：


$$
\pi_i^{(t+1)}=\sum_j P_{i,j}\pi_j^{(t)}
$$


若无论 $\pi^{(t)}$ 的分布如何，在经历上述的随机转移足够多次后，随机变量的取值分布都会最终趋近于一个不变分布P(x)，那么称这个分布为此马尔科夫过程的不变分布。

一个马尔可夫过程最终能趋于不变分布的条件是：这个马尔科夫过程是各态遍历的，具体要求：

1、没有周期性。如果从任意一个状态出发回到自身所需要的转移次数总是一个恒定的次数，那么这个过程就是周期性的。

2、不可约。指的是每一个状态都可能来自任意其他状态。



二、细致平衡条件

对于一个马尔科夫过程，如果满足细致平衡条件，则最终一定能趋于一个不变分布P(x)。

细致平衡条件为，对于任意i,j：


$$
P_{i,j}P(x_j)=P_{j,i}P(x_i)
$$


可以理解为，将大量按照这个马尔科夫过程在各态之间转移的粒子放进这个分布里，则在任意两个态之间往返的粒子流应当是相等的，这样任意态上的粒子数将是一个不变量。注意，细致平衡条件只是不变分布的充分条件，相当于要求状态空间中不存在粒子流，但实际上即使存在粒子流只要其散度是0那么各态的粒子数也是稳恒的。



三、Metropolis-Hasting采样

假设我们要构造使分布P(x)为不变分布的马尔科夫过程，我们尝试构造满足细致平衡的转移概率：


$$
P_{i,j}P(x_i)=P_{j,i}P(x_j)
$$


接下来我们开始设计 $P_{i,j}$ 的值。由于x的维度可能很大，去计算符合上述约束的 $P_{i,j}$ 非常困难，为了让我们可以自由设计转移概率的值而不受上述细致平衡条件的约束，把 $P_{i,j}$ 拆成两部分：


$$
P_{i,j}=\alpha_{i,j}Q_{i,j}
$$


其中 $Q_{i,j}$ 可以自由设计， $\alpha_{i,j}$ 负责矫正概率使之符合细致平衡条件。具体来讲：


$$
\alpha_{i,j}Q_{i,j}P(x_j)=\alpha_{j,i}Q_{j,i}P(x_i)
$$


一种符合上面方程的解同时最大值不会超过1的解为：


$$
\alpha_{i,j}=min(1,\frac{Q_{j,i}P(x_i)}{Q_{i,j}P(x_j)})
$$

$$
\alpha_{j,i}=min(1,\frac{Q_{i,j}P(x_j)}{Q_{j,i}P(x_i)})
$$



这种形式符合细致平衡条件，同时由于天生最大值为1的特征，我们可以认为 $\alpha_{i,j}$ 表示的是某种概率。我们认为它表示的是一个从j到i的转移被接受的概率。也就是说，任意一个转移要想发生首先要经过一个”提议“过程，提议概率为 $Q_{i,j}$ ；接着再通过一个”接受“过程，接受概率为 $\alpha_{i,j}$ 。只有两个过程都通过了才能发生这次转移。

实际操作时，我们将Q取为一些比较简单的概率分布，也就是”提议核“。α将根据提议核的形式计算得到。



四、Gibbs采样

在MH采样中，每次提议转移还需要再通过一定的概率才会被接受，这导致算法收敛很慢，尤其是如果Q的值没有控制好会导致α很小。而Gibbs采样的目的是寻找一个Q的形式使得MH采样中的α恒为1，大大提高了算法效率。

具体方法是通过将P(x)拆成 $P(x)=P(u_1,u_2,...,u_m)$ 的联合分布（注意这里的u并非表示x的一种状态，而是表示x的一个部分或者说一个特征），可以理解为把原状态x拆为很多个分量，例如把一堆不同颜色不同型号的车拆成按颜色分类和按型号分类。然后利用联合概率的公式使α总为1。

在这种拆分下，我们把从任意 $x_j$ 状态到 $x_i$ 状态的转移拆分为m次转移，每次只转移一个分量。也就是我们限制了一步之内所有可能的转移只发生在两个只有一个分量不同的状态之间，可以认为其他有更多分量同时发生变化的转移的概率被我们取为0。在此基础上，对于从 $(u_p,u_{-p})$ 到 $(u_p',u_{-p})$ （-p表示除第p个分量之外其他所有分量）这样的转移，根据联合概率公式有：


$$
P(x)=P(u_p|u_{-p})P(u_{-p})
$$

$$
P(x')=P(u_p'|u_{-p})P(u_{-p})
$$



代入α的表达式：


$$
\alpha_{x',x}=min(1,\frac{Q_{x,x'}P(x')}{Q_{x',x}P(x)})
$$

$$
=min(1,\frac{Q_{x,x'}P(u_p'|u_{-p})P(u_{-p})}{Q_{x',x}P(u_p|u_{-p})P(u_{-p})})
$$

$$
=min(1,\frac{Q_{x,x'}P(u_p'|u_{-p})}{Q_{x',x}P(u_p|u_{-p})})
$$



这样，我们只需令：


$$
Q_{x',x}=P(u_p'|u_{-p})
$$


则α将恒为1。

这个结果的意思是，每次转移一个分量的概率，就相当于在别的分量不变的前提下，重新对这个分量按条件分布做一次采样，这意味着转移概率甚至与转移前的 $u_p$ 无关。从而转移概率可以简单写为：


$$
P_{x',x ; p}=P(u_p'|u_{-p})
$$




五、受限玻尔兹曼机（RBM）

受限玻尔兹曼机是对Gibbs采样的最简单的实现。RBM的目标是拟合一个从真实世界采样的向量的概率分布，是一种无监督学习模型。具体来说，RBM是将Gibbs采样中的m取为2，也就是每个状态只拆为两个分量来考虑。

RBM中这两个分量记为v和h，其中我们希望通过训练使得v的分布接近真实分布，而h作为隐藏向量，是模型的副产物。v和h都用二进制编码的向量来表示，每个维度取值只有{0,1}。

RBM名称中所谓”受限“，指的是v和h向量内部的每个维度之间的概率分布互相独立。也就是说，只要固定向量v之后，h的某一维为1（称为激活）的概率就固定了，与其他维无关，反之亦然，从而：（注意这里的i是向量的指标，而非第i时刻）


$$
P(v|h)=\prod_iP(v_i|h)
$$

$$
P(h|v)=\prod_iP(h_i|v)
$$



RBM从分布中采样的方式也和Gibbs采样的方式相同，每一步只改变v或h向量中的一个。我们可以认为一次大转移分为两次小转移，即v转移后h转移：


$$
P(v,h')=P(h'|v)
$$

$$
P(v',h')=P(v'|h')
$$



由于独立性（即所谓”受限“）：


$$
P(v,h')=\prod_i P(h'_i|v)
$$

$$
P(v',h')=\prod_i P(v'_i|h')
$$



也就是我们只要能计算出任意v,h的条件概率 $P(h_i|v)$ 就可以按上述过程开始采样，并在足够多次迭代后得到分布趋近P(v,h)的采样结果。

为了更好的参数化概率分布函数，我们用定义能量函数的方式来借助玻尔兹曼分布律间接导出概率分布函数。对于一组给定状态(v,h)，可定义如下能量函数：


$$
E_\theta(v,h)=-a^Tv-b^Th-h^TWv
$$


可以理解为两个向量的自能叠加两个向量之间的互能，并且每个向量内部是去耦合的。利用玻尔兹曼分布律，可以得到(v,h)的联合概率分布：


$$
P_\theta(v,h)=\frac{1}{Z_\theta}e^{-E_\theta(v,h)}
$$


其中Z为配分函数（归一化因子）：


$$
Z_\theta=\sum_{v,h}e^{-E_\theta(v,h)}
$$


可见，得到的概率分布保证v,h各自的每一维之间是独立的，而两个向量的任意维度之间都是耦合的。如果画出概率图，得到的将是一个二分图。

接下来，计算 $P(h_i|v)$ ，由于 $h_i$ 只有{0,1}两种取值，我们计算 $h_i=1$ 也即这一维被激活的概率。


$$
\begin{aligned}
P(h_i=1|v)
&=P(h_i=1|v,h_{-i})（由独立性）\\
&=\frac{P(v,h_i=1)}{P(v)}\\
&=\frac{P(v,h_i=1)}{P(v,h_i=1)+P(v,h_i=0)}\\
&=\frac{1}{1+\frac{P(v,h_i=0)}{P(v,h_i=1)}}\\
\end{aligned}
$$


其中：


$$
\frac{P(v,h_i=0)}{P(v,h_i=1)}=\frac{e^{-E_\theta(v,h_i=0)}} {e^{-E_\theta(v,h_i=1)}}
$$


由于除了 $h_i$ 之外的所有维度都是不变的，在分式中被消去：


$$
\frac{P(v,h_i=0)}{P(v,h_i=1)}=e^{-b_i-\sum_j W_{i,j}v_j}
$$


定义：


$$
\alpha_i(v)=b_i+\sum_j W_{i,j}v_j
$$


则：


$$
P(h_i=1|v)=\frac{1}{1+e^{-\alpha_i(v)}}=sigmoid(\alpha_i(v))
$$


综上，我们得到了RBM的状态转移方式和转移概率的具体计算结果。接下来我们研究RBM如何训练优化其参数 $a,b,W$ 。



六、受限玻尔兹曼机（RBM）的训练

训练RBM的方式是通过最大化似然函数 $P(v_0)$ （这里的 $v_0$ 表示一组输入数据），具体方法为梯度下降优化负对数似然损失：


$$
L=-\log P(v_0)
$$


计算梯度可以得到：


$$
\frac{\partial L}{\partial W_{i,j}}=\sum_v P(v)P(h_i=1|v)v_j-P(h_i=1|v_0)v_{0j}
$$

$$
\frac{\partial L}{\partial a_i}=\sum_v P(v)v_i-v_{0i}
$$

$$
\frac{\partial L}{\partial b_i}=\sum_vP(v)P(h_i=1|v)-P(h_i=1|v_0)
$$



但这种方法对v求和的复杂度是 $2^{n_v}$ 级别的，从而计算的难度非常大，接下来我们来看一种效率更高的训练算法。



七、对比散度算法
