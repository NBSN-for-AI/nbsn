估计：模型给出一个概率分布来估计真实数据的分布。

对于无监督学习，模型估计概率P(x)。

对于有监督学习，模型估计概率P(y|x)。即认为对于每个输入数据，输出的应是概率分布。例如对于分类问题，输出的是不同标签的离散的概率分布；对于回归问题，输出的是回归的值的连续概率分布。

对于模型 $p_{model}(x)$ 中某些参量的最优估计称为点估计，例如估计一个高斯分布的均值和方差。把要确定的参量抽象为向量 $\hat\theta_m$ ，则其由所有训练数据确定：


$$
\hat\theta_m=g(X)
$$


按照频率学派的观点， $\hat\theta$ 由x唯一确定，对应的估计方式有最大似然估计：


$$
\hat\theta_{ML}=\arg\max_{\theta} p_{model}(X;\theta)
$$

$$
=\arg\max_\theta \prod_{i=1}^m p_{model}(x_i;\theta)
$$



为了便于计算防止数值下溢，取对数：


$$
\hat\theta_{ML}=\arg\max_\theta \sum_{i=1}^m \log p_{model}(x_i;\theta)
$$

$$
=\arg\min_\theta \sum_{i=1}^m -\log p_{model}(x_i;\theta)
$$



这样我们就得到了需要优化的损失函数：负对数似然。

扩展到估计条件概率（有监督问题）P(y|x)，最大似然为：


$$
\hat\theta_{ML}=\arg\min_\theta \sum_{i=1}^m -\log p_{model}(y_i|x_i;\theta)
$$


即交叉熵损失函数。

若假设 $p_{model}(y|x;\theta)=N(y;\hat y(x),\sigma^2)$ ，即假设对同一个输入x，y符合对x各个特征具有相同方差的高斯分布，则：


$$
\hat\theta_{ML}=\arg\min_\theta(m\log\sigma+\frac{m}{2}\log(2\pi)+\sum_{i=1}^m \frac{||y_i-\hat y_i||^2}{2\sigma^2})
$$


即均方误差MSE损失函数：


$$
MSE_{train}=\arg\min_\theta \frac{1}{m} \sum_{i=1}^m ||y_i-\hat y_i||^2
$$


而按照贝叶斯学派的观点，模型的参数并非一个固定的最优值，而是存在一个分布，在预测时也是对不同参数按概率求期望给出的。并且，我们认为参数的分布应该存在一个先验分布 $p(\theta)$ ，在训练过程中不断由训练数据进行后验确定最终分布 $p(\theta|X)$ ：


$$
p(\theta|X) = \frac{p(X|\theta) p(\theta)}{p(X)}
$$


但用完整的后验分布进行预测不现实，我们用后验分布中最大后验点 $\theta_{MAP}$ 来近似估计，即最大后验估计：


$$
\theta_{MAP}=\arg\max_\theta \sum_{i=1}^m\log p_{model}(x_i;\theta)+\log p(\theta)
$$

$$
=\arg\min_\theta \sum_{i=1}^m-\log p_{model}(x_i;\theta)-\log p(\theta)
$$



条件概率：


$$
\theta_{MAP}=\arg\min_\theta \sum_{i=1}^m-\log p_{model}(y_i|x_i;\theta)-\log p(\theta)
$$


可见与频率学派最大似然公式的差别只有第二项。

以权重为w线性回归模型为例，对于高斯先验分布 $N(w;0,\frac{1}{\lambda}I^2)$ ，第二项的形式为 $\lambda w^Tw$ ，对应 $L^2$ 正则化。